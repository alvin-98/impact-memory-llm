{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03c88fbf-abfe-4601-befc-1f03a99c3435",
   "metadata": {},
   "source": [
    "My project explores how different types of memory affect the performance of intelligent code-generating agents in a collaborative software development environment. The environment simulates a real-world development pipeline with three key agent roles: (1) Code Agent, which writes and iterates on code until tests pass, (2) Review Agent, which generates new test cases to ensure acceptance criteria are met, and (3) Orchestrator/Product Manager, which converts user requests into structured user stories, writes test cases, and defines acceptance criteria.\n",
    "\n",
    "The central research question is: How does the type of memory used by agents impact the quality and efficiency of code development?\n",
    "\n",
    "We will compare three memory setups: (1) basic memory (tracking past actions), (2) summarised \"learnings\" generated by a language model from past interactions, and (3) agentic memory (a-mem), which allows more advanced, context-aware recall. Agents will be evaluated across metrics such as the number of bugs found, number of errors encountered, iteration counts, and overall rate of improvement.\n",
    "\n",
    "Experiments will be conducted across tasks starting with mostly basic python problem dataset from google and humaneval from openai to measure performance variations. The results could provide insights into how memory mechanisms can be optimized for multi-agent collaboration in intelligent systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee28a34-a98e-43d5-ae99-ef80c2e15f58",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "CODE AGENT \n",
    "- Writes only code given task\n",
    "\n",
    "REVIEW AGENT\n",
    "- Writes tests for given task and checks if code passes / fails\n",
    "\n",
    "ORCHESTRATOR / PM\n",
    "- converts user requests into structured prompts\n",
    "- Performs final tests using acceptance criteria\n",
    "\n",
    "Note: Acceptance criteria is derived from MBPP test_list\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d5677b8-09a6-46c0-b558-3578f2d72603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, io, datetime\n",
    "from contextlib import redirect_stdout\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from google import genai\n",
    "from ollama import chat, ChatResponse\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Callable\n",
    "\n",
    "# load both Gemini and Ollama settings\n",
    "load_dotenv()\n",
    "MODEL_BACKEND = os.getenv(\"MODEL_BACKEND\", \"gemini\")      # or \"ollama\"\n",
    "GEMINI_MODEL  = os.getenv(\"GEMINI_MODEL\",  \"gemini-2.5-pro-exp-03-25\")\n",
    "OLLAMA_MODEL  = os.getenv(\"OLLAMA_MODEL\",  \"gemma3:1b\")\n",
    "\n",
    "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ed6d463c-7b3c-47a8-bd83-024ef5b70adf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9803ff2-d9d9-4ff2-b961-007c970edc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)  # or use 0 for unlimited in newer versions\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12c3e729-467f-4ade-aaab-2598278d3809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['task_id', 'text', 'code', 'test_list', 'test_setup_code', 'challenge_test_list'],\n",
       "        num_rows: 974\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the MBPP dataset\n",
    "mbpp_dataset = load_dataset(\"Muennighoff/mbpp\")\n",
    "\n",
    "mbpp_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c022c18-bc85-47b0-a898-120157736cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbpp_data = mbpp_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7904f88-6e3f-4fb0-87da-400b1d43b9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbpp_df = pd.DataFrame(mbpp_data)\n",
    "task_1_df = mbpp_df[mbpp_df['task_id'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1757202-756e-4a4e-91eb-62c55ad950b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['assert min_cost([[1, 2, 3], [4, 8, 2], [1, 5, 3]], 2, 2) == 8',\n",
       " 'assert min_cost([[2, 3, 4], [5, 9, 3], [2, 6, 4]], 2, 2) == 12',\n",
       " 'assert min_cost([[3, 4, 5], [6, 10, 4], [3, 7, 5]], 2, 2) == 16']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_1_df['test_list'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00dc7a08-9c43-4b1b-b759-1f7e389db610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Write a function to find the minimum cost path to reach (m, n) from (0, 0) for the given cost matrix cost[][] and a position (m, n) in cost[][].\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# ORCHESTRATOR / PM\n",
    "\n",
    "print(task_1_df.text)\n",
    "\n",
    "task = task_1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1184b5a-fefd-4162-96b4-554d8454d4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8245dad5-d08f-453b-995e-38fdec1263b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Write a function to find the minimum cost path to reach (m, n) from (0, 0) for the given cost matrix cost[][] and a position (m, n) in cost[][].'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "206af9b7-d611-40dd-bc11-bf073960497a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CodeAgentResponse(BaseModel):\n",
    "    function_name: str\n",
    "    code: str\n",
    "\n",
    "    @property\n",
    "    def function(self) -> Callable:\n",
    "        namespace: dict = {}\n",
    "        exec(self.code, namespace)\n",
    "        func = namespace.get(self.function_name)\n",
    "        if func is None or not callable(func):\n",
    "            raise ValueError(f\"Function {self.function_name} not found after exec.\")\n",
    "        return func\n",
    "\n",
    "class ReviewAgentResponse(BaseModel):\n",
    "    test_cases: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb8da1da-f2d0-47a0-9cac-917f99876cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_agent(requirement: str) -> CodeAgentResponse:\n",
    "    if MODEL_BACKEND == \"gemini\":\n",
    "        resp = client.models.generate_content(\n",
    "            model=GEMINI_MODEL,\n",
    "            contents=requirement,\n",
    "            config={\n",
    "                \"response_mime_type\": \"application/json\",\n",
    "                \"response_schema\": CodeAgentResponse,\n",
    "            },\n",
    "        )\n",
    "        return resp.parsed\n",
    "\n",
    "    elif MODEL_BACKEND == \"ollama\":\n",
    "        prompt = (\n",
    "            requirement + \"\\n\\nUse this JSON schema:\\n\"\n",
    "            \"CodeAgent = {'function_name': str, 'code': str}\\nReturn: CodeAgent\"\n",
    "        )\n",
    "        resp: ChatResponse = chat(\n",
    "            model=OLLAMA_MODEL,\n",
    "            messages=[{\"role\":\"user\",\"content\":prompt}],\n",
    "        )\n",
    "        return CodeAgentResponse.parse_raw(resp.message.content)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown MODEL_BACKEND: {MODEL_BACKEND}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "082cc208-aa0e-41af-ab36-eae3d7041868",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_agent_response = code_agent(task.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e20186c4-c2af-47d2-88b9-7c42ac76147a",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_cost_fn = code_agent_response.function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b65b3c8-4bf5-4a08-a115-73e955c80a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def min_cost_path(cost, m, n):\\n    \"\"\"Finds the minimum cost path to reach (m, n) from (0, 0) in the given cost matrix.\\n\\n    Args:\\n        cost: A 2D list representing the cost matrix.\\n        m: The row index of the destination.\\n        n: The column index of the destination.\\n\\n    Returns:\\n        The minimum cost to reach (m, n) from (0, 0).\\n    \"\"\"\\n    dp = [[0 for _ in range(n + 1)] for _ in range(m + 1)]\\n\\n    dp[0][0] = cost[0][0]\\n\\n    # Initialize first column\\n    for i in range(1, m + 1):\\n        dp[i][0] = dp[i-1][0] + cost[i][0]\\n\\n    # Initialize first row\\n    for j in range(1, n + 1):\\n        dp[0][j] = dp[0][j-1] + cost[0][j]\\n\\n    # Construct the rest of the DP table\\n    for i in range(1, m + 1):\\n        for j in range(1, n + 1):\\n            dp[i][j] = cost[i][j] + min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1])\\n\\n    return dp[m][n]'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_agent_response.code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3febd410-b7b0-49a2-9e60-dfbb707cad35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function min_cost_path(cost, m, n)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_cost_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc2c51b3-bc5e-43f1-9e9e-8699a274ab39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_agent(requirement: str, code: str) -> ReviewAgentResponse:\n",
    "    if MODEL_BACKEND == \"gemini\":\n",
    "        prompt = (\n",
    "            \"You are a review agent. ONLY respond with JSON containing 'test_cases' as a list of Python assert statements. NO commentary.\"\n",
    "            f\"\\nRequirement: {requirement}\"\n",
    "            f\"\\nFunction code:\\n{code}\"\n",
    "        )\n",
    "        resp = client.models.generate_content(\n",
    "            model=GEMINI_MODEL,\n",
    "            contents=prompt,\n",
    "            config={\n",
    "                \"response_mime_type\": \"application/json\",\n",
    "                \"response_schema\": ReviewAgentResponse,\n",
    "            },\n",
    "        )\n",
    "        return resp.parsed\n",
    "\n",
    "    elif MODEL_BACKEND == \"ollama\":\n",
    "        prompt = (\n",
    "            requirement + \"\\n\\nUse this JSON schema:\\n\"\n",
    "            \"ReviewAgent = {'test_cases': list[str]}\\nReturn: ReviewAgent\"\n",
    "        )\n",
    "        resp: ChatResponse = chat(\n",
    "            model=OLLAMA_MODEL,\n",
    "            messages=[{\"role\":\"user\",\"content\":prompt}],\n",
    "        )\n",
    "        return ReviewAgentResponse.parse_raw(resp.message.content)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown MODEL_BACKEND: {MODEL_BACKEND}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74feb262-f353-4064-899e-a3b6ebca66ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "requirement = task.text[0]\n",
    "review_agent_response = review_agent(requirement, code_agent_response.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cab23877-8b88-4fbc-bfc9-662e14dc356d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReviewAgentResponse(test_cases=['assert min_cost_path([[1, 2, 3], [4, 8, 2], [1, 5, 3]], 2, 2) == 8', 'assert min_cost_path([[1, 2, 3], [4, 8, 2], [1, 5, 3]], 1, 1) == 9', 'assert min_cost_path([[1, 2, 3], [4, 8, 2], [1, 5, 3]], 0, 0) == 1', 'assert min_cost_path([[1, 2, 3], [4, 8, 2], [1, 5, 3]], 0, 1) == 3', 'assert min_cost_path([[1, 2, 3], [4, 8, 2], [1, 5, 3]], 1, 0) == 5', 'assert min_cost_path([[5]], 0, 0) == 5', 'assert min_cost_path([[1,2],[3,4]],1,1) == 8', 'assert min_cost_path([[4,5,6],[1,2,3],[7,8,9]],2,2) == 21'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_agent_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea5a8584-c01f-45e9-aadd-aaf5be08e0dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['assert min_cost_path([[1, 2, 3], [4, 8, 2], [1, 5, 3]], 2, 2) == 8',\n",
       " 'assert min_cost_path([[1, 2, 3], [4, 8, 2], [1, 5, 3]], 1, 1) == 9',\n",
       " 'assert min_cost_path([[1, 2, 3], [4, 8, 2], [1, 5, 3]], 0, 0) == 1',\n",
       " 'assert min_cost_path([[1, 2, 3], [4, 8, 2], [1, 5, 3]], 0, 1) == 3',\n",
       " 'assert min_cost_path([[1, 2, 3], [4, 8, 2], [1, 5, 3]], 1, 0) == 5',\n",
       " 'assert min_cost_path([[5]], 0, 0) == 5',\n",
       " 'assert min_cost_path([[1,2],[3,4]],1,1) == 8',\n",
       " 'assert min_cost_path([[4,5,6],[1,2,3],[7,8,9]],2,2) == 21']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_agent_response.test_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "078c2518-28c5-491e-98ab-cd012b9b3fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged 8 tests to test_logs.csv\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "ns = {code_agent_response.function_name: code_agent_response.function}\n",
    "for tc in review_agent_response.test_cases:\n",
    "    buf = io.StringIO()\n",
    "    try:\n",
    "        with redirect_stdout(buf):\n",
    "            exec(tc, ns)\n",
    "        status, err = \"pass\", \"\"\n",
    "    except AssertionError as e:\n",
    "        status, err = \"fail\", str(e)\n",
    "    except Exception as e:\n",
    "        status, err = \"error\", str(e)\n",
    "    results.append({\n",
    "        \"requirement\": requirement,\n",
    "        \"fn_name\": code_agent_response.function_name,\n",
    "        \"test\": tc,\n",
    "        \"status\": status,\n",
    "        \"error\": err,\n",
    "        \"stdout\": buf.getvalue(),\n",
    "        \"timestamp\": datetime.datetime.now(),\n",
    "    })\n",
    "\n",
    "df_logs = pd.DataFrame(results)\n",
    "log_file = \"test_logs.csv\"\n",
    "if os.path.exists(log_file):\n",
    "    df_logs.to_csv(log_file, mode=\"a\", header=False, index=False)\n",
    "else:\n",
    "    df_logs.to_csv(log_file, index=False)\n",
    "\n",
    "print(f\"Logged {len(results)} tests to {log_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553d28f0-8f70-49e9-b5b8-220a0aa86222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters (inâ€notebook instead of argparse)\n",
    "eval_type = \"mbpp\"      # or \"humaneval\"\n",
    "num_tasks = 10          # or None for all\n",
    "\n",
    "if eval_type==\"mbpp\":\n",
    "    ds = load_dataset(\"Muennighoff/mbpp\")[\"test\"]\n",
    "    get_tests = lambda r: r[\"test_list\"]\n",
    "    get_req   = lambda r: r[\"text\"]\n",
    "else:\n",
    "    ds = load_dataset(\"openai/openai_humaneval\")[\"test\"]\n",
    "    get_tests = lambda r: r[\"test_list\"]\n",
    "    get_req   = lambda r: r[\"prompt\"]\n",
    "\n",
    "logs = []\n",
    "for idx, rec in enumerate(ds):\n",
    "    if num_tasks and idx>=num_tasks: break\n",
    "    req = get_req(rec)\n",
    "    code_r = code_agent(req)\n",
    "    rev_r  = review_agent(req, code_r.code)\n",
    "    fn     = code_r.function\n",
    "    ns     = {code_r.function_name: fn}\n",
    "\n",
    "    ds_pass = ds_fail = 0\n",
    "    for tc in get_tests(rec):\n",
    "        try: exec(tc, ns); ds_pass+=1\n",
    "        except: ds_fail+=1\n",
    "\n",
    "    logs.append({\n",
    "      \"eval_type\":eval_type,\n",
    "      \"task_id\":   rec.get(\"task_id\",idx),\n",
    "      \"requirement\":req,\n",
    "      \"dataset_pass\":ds_pass,\n",
    "      \"dataset_fail\":ds_fail,\n",
    "      \"timestamp\": datetime.datetime.now(),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(logs)\n",
    "fname = \"orchestrator_logs.csv\"\n",
    "df.to_csv(fname, index=False, mode=\"a\", header=not os.path.exists(fname))\n",
    "print(f\"Saved {len(logs)} rows to {fname}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
